[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The 2026 ACIC Data Challenge",
    "section": "",
    "text": "After a multi-year hiatus, we are excited to announce the return of the ACIC Data Challenge. This year the challenge will focus on the complications that arise in situations with multiple treatments. This will help us to understand how causal inference approaches handle the competing demands of settings with a wide variety of estimands. What’s the best approach to avoiding overconfidence in situations involving multiple, dependent comparisons? Can the same method yield strong results for both ATEs and subgroup effects? What are the tradeoffs between accurately estimating the effects of different treatments versus determining which of those treatments is most efficacious? Do effect rankings suggest a different approach than accurate effect estimates? We hope to address questions like these and more!\nIn addition, a key goal this year is to create a more inclusive competition by lowering the barriers to entry with regard to computing power and level of automation. To that end our datasets will be reasonably small (likely no more than 5000 observations). Moreover, for those who prefer more curated approaches to causal inference we will provide opportunities to make submissions only for a core subset of datasets or estimands.\nWe will release the data on February 1, 2026. Submissions will be due on April 20, 2026. We will announce the results at ACIC in May!"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "The 2026 ACIC Data Challenge",
    "section": "",
    "text": "After a multi-year hiatus, we are excited to announce the return of the ACIC Data Challenge. This year the challenge will focus on the complications that arise in situations with multiple treatments. This will help us to understand how causal inference approaches handle the competing demands of settings with a wide variety of estimands. What’s the best approach to avoiding overconfidence in situations involving multiple, dependent comparisons? Can the same method yield strong results for both ATEs and subgroup effects? What are the tradeoffs between accurately estimating the effects of different treatments versus determining which of those treatments is most efficacious? Do effect rankings suggest a different approach than accurate effect estimates? We hope to address questions like these and more!\nIn addition, a key goal this year is to create a more inclusive competition by lowering the barriers to entry with regard to computing power and level of automation. To that end our datasets will be reasonably small (likely no more than 5000 observations). Moreover, for those who prefer more curated approaches to causal inference we will provide opportunities to make submissions only for a core subset of datasets or estimands.\nWe will release the data on February 1, 2026. Submissions will be due on April 20, 2026. We will announce the results at ACIC in May!"
  },
  {
    "objectID": "index.html#data-background",
    "href": "index.html#data-background",
    "title": "The 2026 ACIC Data Challenge",
    "section": "Data Background",
    "text": "Data Background\nFor the Data Challenge, we have created 9000 different datasets, each of which represents a simple random sample from different, much larger populations. These populations differ not only in terms of the treatment assignment probabilities but also in terms of the size of the treatment effects and the complexity/heterogeneity/functional form of the (conditional) average treatment effect functions.\nEach dataset contains \\(n\\) triplets \\((\\boldsymbol{\\mathbf{x}}_{i}, z_{i}, y_{i})\\) of $p = $ covariates \\(\\boldsymbol{\\mathbf{X}}\\), treatment indicator \\(Z \\in \\{A, B, C, D, E\\},\\) and outcome \\(Y.\\) You may assume that these triplets were drawn independently and uniformly at random from a much larger population in which treatment was assigned completely at random. So, standard assumptions of ignorability and overlap are satisfied. The Stable Unit Treatment Value Assumption (SUTVA) can also be assumed to be satisfied.\n\nCovariates\nThere are continuous, binary, and nominal covariates. You may assume that the distributions of the covariates are the same across all populations considered in this challenge.\n\n\nTarget Estimands\nAcross all populations, you may assume that \\(A\\) represents the control or “business as usual” condition and that \\(B, C, D, E\\) represent different treatment arms. We will be interested in estimating the average effect of each treatment relative to \\(A\\) at three levels: within subgroups defined by all the covariates, within subgroups defined by a single, binary covariate, and the overall population level. We will also be interested in determining which treatment is the most efficacious relative to \\(A\\) at each level, where you may assume that positive treatment effects are considered better than negative effects.\nFormally, each individual in the population has five potential outcomes \\(Y(A), Y(B), Y(C), Y(D), Y(E)\\), out of which exactly one is observed: \\(y = Y(Z).\\) For each \\(z \\in \\{B, C, D, E\\}\\) and covariate vector \\(\\boldsymbol{\\mathbf{x}},\\) the conditional average treatment effect function is defined as \\[\n\\textrm{CATE}(\\boldsymbol{\\mathbf{x}}, z) = \\mathbb{E}[Y(z) - Y(A) \\vert \\boldsymbol{\\mathbf{X}} = \\boldsymbol{\\mathbf{x}}].\n\\] We are also interested in the effect of each treatment in \\(\\{B, C, D, E\\}\\) (relative to \\(A\\)) within the sub-population defined by \\(X_{2} = .\\) \\[\n\\textrm{GATE}(z) = \\mathbb{E}[Y(z) - Y(A) \\vert X_{2} = ]\n\\]\nFinally, we are interested in the overall population-level average treatment effect \\[\n\\textrm{PATE}(z) = \\mathbb{E}[Y(z) - Y(A)]\n\\]\nIn addition to estimating these average effect functions, we will be interested in determining which treatment in \\(\\{B, C, D, E\\}\\) has the largest effect at all three levels. Specifically, we wish to compute\n\nFor each observed covariate vector \\(\\boldsymbol{\\mathbf{x}}\\), \\(\\textrm{argmax}_{z} \\textrm{CATE}(\\boldsymbol{\\mathbf{x}}_{i}, z).\\)\nWithin the subgroup defined by $X_{2} = $, \\(\\textrm{argmax}_{z} \\textrm{GATE}(z).\\)\nAcross the whole population \\(\\textrm{argmax}_{z} \\textrm{ATE}(z).\\)"
  },
  {
    "objectID": "index.html#submission-instructions",
    "href": "index.html#submission-instructions",
    "title": "The 2026 ACIC Data Challenge",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nFor each dataset, you will submit a separate CSV file containing estimates and, in the case of the \\(\\textrm{CATE}, \\textrm{GATE},\\) and \\(\\textrm{PATE}\\) evaluations, uncertainty intervals. Refer to the following sections for details about how to name and organize these files. For all estimands and datasets, you will need to include the following information in the submission filenames:\n\ndataID: dataset ID. Each dataset is stored in a file named something like \"data_dataID.csv\", where dataID is a 4 digit number running from 0001 to 9000.\nteamID: a unique ID number assigned to each submission team. Team numbers will be assigned no later than April 1, 2026 by the organizers.\nsubmissionID: if you enter multiple submissions, you will need to include a numeric id so that we can tell your submissions apart. See below for more details about multiple submissions.\n\n\n\\(\\textrm{CATE}\\) Evaluations\nFor each dataset, submit a single CSV file containing point estimates and uncertainty intervals for the conditional average treatment effect functions evaluated at each observed covariate vector. These outputs should be arranged in “long” format with the following five named columns:\n\n“ID”: the observation identifier (numeric)\n“z”: treatment arm (i.e., “B”, “C”, “D”, or “E”) (character)\n“Estimate”: the point estimate for \\(\\textrm{CATE}(\\boldsymbol{\\mathbf{x}}_{\\textrm{ID}}, \\textrm{z}).\\)\n“L95”: the lower bound for a 95% uncertainty interval for the corresponding CATE evaluation\n“U95”: the upper bound for a 95% uncertainty interval for the corresponding CATE evaluation\n\nThe CSV file should be named \"CATE_dataID_teamID_submissionID.csv\". If the dataset contains \\(n\\) observation, this file should contain \\(4n + 1\\) rows, where the first row contains the header/column names.\n\n\n\\(\\textrm{GATE}\\) Evaluations\nFor each dataset, submit a single CSV file containing point estimates and uncertainty intervals for the average treatment effect within subgroups defined by \\(X_{2} = .\\) These outputs should be arranged in “long” format with the following named columns\n\n“z”: treatment arm (i.e., “B”, “C”, “D”, or “E”) (character)\n“Estimate”: the point estimate for \\(\\textrm{GATE}(\\textrm{z}).\\)\n“L95”: the lower bound for a 95% uncertainty interval for the corresponding GATE evaluation\n“U95”: the upper bound for a 95% uncertainty interval for the corresponding GATE evaluation\n\nThe CSV file should be named \"GATE_dataID_teamID_submissionID.csv\" and should only have 5 rows (including the header/column names).\n\n\n\\(\\textrm{PATE}\\) Evaluations\nFor each dataset, submit a single CSV file containing point estimates and uncertainty intervals for the population average treatment effect. These outputs should be arranged in “long” format with the following columns\n\n“z”: treatment arm (i.e., “B”, “C”, “D”, or “E”) (character)\n“Estimate”: the point estimate for \\(\\textrm{GATE}(\\textrm{z}).\\)\n“L95”: the lower bound for a 95% uncertainty interval for the corresponding GATE evaluation\n“U95”: the upper bound for a 95% uncertainty interval for the corresponding GATE evaluation\n\nThe CSV file should be named \"GATE_dataID_teamID_submissionID.csv\" and should only have 5 rows (including the header/column names).\n\n\nBest \\(\\textrm{CATE}\\)\nFor each dataset, submit a single CSV file identifying the treatment arm that is most efficacious relative to the business as usual/control condition \\(A\\) within subgroups of the population defined by each observed covariate vector \\(\\boldsymbol{\\mathbf{x}}_{i}.\\) These outputs should be arranged in “long” format with the following columns:\n\n“ID”: the observation identifier (numeric)\n“best_z”: treatment arm \\(z\\) (i.e., “B”, “C”, “D”, or “E”) that maximizes \\(\\textrm{CATE}(\\boldsymbol{\\mathbf{x}}_{\\textrm{ID}}, z)\\) (character)\n\nThe CSV file should be named \"BEST_CATE_dataID_teamID_submission_ID.csv\". The dataset contains \\(n\\) observations, the submission file should have \\(n+1\\) rows (including the header/column names).\n\n\nBest \\(\\textrm{GATE}\\)\nFor each dataset, submit a single CSV file identifying the treatment arm that is most efficacious relative to the business as usual/control condition \\(A\\) within subgroups of the population defined \\(X_{2} = .\\) These outputs should be arranged in “long” format with the following columns:\n\n“best_z”: treatment arm \\(z\\) (i.e., “B”, “C”, “D”, or “E”) that maximizes \\(\\textrm{GATE}(z)\\) (character)\n\nThe CSV file should be named \"BEST_GATE_dataID_teamID_submission_ID.csv\" and should include 2 rows (including the header/column names).\n\n\nBest \\(\\textrm{PATE}\\)\nFor each dataset, submit a single CSV file identifying the treatment arm that is most efficacious relative to the business as usual/control condition \\(A\\) within subgroups of the population defined \\(X_{2} = .\\) These outputs should be arranged in “long” format with the following columns:\n\n“best_z”: treatment arm \\(z\\) (i.e., “B”, “C”, “D”, or “E”) that maximizes \\(\\textrm{PATE}(z)\\) (character)\n\nThe CSV file should be named \"BEST_PATE_dataID_teamID_submission_ID.csv\" and should include 2 rows (including the header/column names).\nExamples of different submission files are available here."
  }
]